"""Implementation of the T5 Model for Response Generation and Question
Generation Used for relation extraction."""

import gc
import math
import os
import random
from dataclasses import dataclass
from typing import Any, Optional

import numpy
import torch
from transformers import Adafactor, T5ForConditionalGeneration, T5Tokenizer


@dataclass
class HyperParameters:
    """General Model configuration."""

    model_path: Optional[str] = None
    batch_size: int = 64
    source_max_length: int = 512
    decoder_max_length: int = 128
    config_file: str = "config.ini"
    dim_embedding: int = 100
    dim_model: int = 128
    dropout: float = 0.5
    gpu: bool = False
    l2_norm_weight: float = 0.01
    learning_rate: float = 0.0005
    max_epochs: int = 16
    mode: str = "train"
    train: Optional[str] = None
    prediction_file: Optional[str] = None
    seed: int = 8
    test: Optional[str] = None
    dev: Optional[str] = None
    answer_checkpoint: Optional[str] = "_3_model"
    question_checkpoint: Optional[str] = "_3_model"
    checkpoint: Optional[str] = "_3_model"

    # Related to beam search decoding.
    beam_decoding: Optional[bool] = False
    num_beams: Optional[int] = 5
    num_beam_groups: Optional[int] = 4
    beam_diversity_penalty: Optional[float] = 0.5
    no_repeat_ngram_size: Optional[int] = 2
    early_stopping: Optional[bool] = True
    question_training_steps: Optional[int] = 5
    answer_training_steps: Optional[int] = 1


def tuple_of_tensors_to_tensor(tuple_of_tensors):
    return torch.stack(list(tuple_of_tensors), dim=0)


def set_random_seed(seed: int) -> Any:
    """Set the random seed, which initializes the random number generator.

    Ensures that runs are reproducible and eliminates differences due to
    randomness.
    """
    random.seed(seed)
    numpy.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def remove_prefix(text, prefix):
    if text.startswith(prefix):
        return text[len(prefix) :]
    return text  # or whatever


def torch_save(model: torch.nn.Module, path: str) -> None:
    """Save the model to task at the specified path."""
    torch.save(model.state_dict(), path)


def save(model, model_path: str, checkpoint_name: str):
    """Save the model to the specified path name."""
    torch_save(model, model_path + "_" + checkpoint_name)


def load_module(model, model_path, checkpoint_name):
    # Load the model from the checkpoint.
    loaded_weights = torch.load(
        model_path + checkpoint_name,
        map_location=lambda storage, loc: storage,
    )
    new_weights = {}
    for key, val in loaded_weights.items():
        new_weights[remove_prefix(key, "module.")] = val
    model.load_state_dict(new_weights)


def clear_cache():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()


def prob_of_sampled_predictions(loss_fct, sample_outputs):
    """Helper function to compute the predictions and the probability of a
    sampled sequence from the output of the generate function in the
    transformers library."""
    # Skip the first pad token generated by the T5 model.
    sampled_predictions = sample_outputs.sequences[:, 1:]
    sampled_scores = sample_outputs.scores
    sampled_scores = tuple_of_tensors_to_tensor(sampled_scores)
    l, n, v = sampled_scores.size()
    log_p = -loss_fct(
        sampled_scores.view(-1, v),
        torch.reshape(torch.transpose(sampled_predictions, 0, 1), (l * n,)),
    ).view(l, n)
    pad_mask = torch.transpose(sampled_predictions, 0, 1) == 0
    good_log_p = log_p.masked_fill_(pad_mask, 0.0)
    log_p = torch.sum(good_log_p, dim=0).squeeze()
    # sampled_p = torch.exp(log_p)
    return sampled_predictions, log_p


MODEL_NAME = "t5-base"
# MODEL_NAME = "allenai/unifiedqa-t5-base"
Q_MODEL_NAME = "iarfmoose/t5-base-question-generator"


class REQA(torch.nn.Module):
    """Wrapper class around the T5 Model."""

    def __init__(self, cfg: HyperParameters, load_answer=True):
        super(REQA, self).__init__()
        self.config = cfg

        set_random_seed(cfg.seed)

        # Answer model
        answer_tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

        # Construct the answer model
        answer_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)

        # Question Model
        question_tokenizer = T5Tokenizer.from_pretrained(Q_MODEL_NAME)

        # Construct the question model
        question_model = T5ForConditionalGeneration.from_pretrained(Q_MODEL_NAME)

        if cfg.mode == "train":
            # Configurations suggested by the T5 paper.
            self.answer_optimizer = Adafactor(
                answer_model.parameters(),
                lr=cfg.learning_rate,
                eps=(1e-30, 1e-3),
                clip_threshold=1.0,
                decay_rate=-0.8,
                beta1=None,
                weight_decay=0.0,
                relative_step=False,
                scale_parameter=False,
                warmup_init=False,
            )

            # Configurations suggested by the T5 paper.
            self.question_optimizer = Adafactor(
                question_model.parameters(),
                lr=cfg.learning_rate,
                eps=(1e-30, 1e-3),
                clip_threshold=1.0,
                decay_rate=-0.8,
                beta1=None,
                weight_decay=0.0,
                relative_step=False,
                scale_parameter=False,
                warmup_init=False,
            )

            if not os.path.exists(cfg.model_path):
                os.makedirs(cfg.model_path)
            self.model_path = os.path.join(cfg.model_path, "model")

            if load_answer:
                # we have a pre-trained answer module.
                load_module(answer_model, self.model_path, cfg.answer_checkpoint)

        elif cfg.mode in ["test", "inference"]:
            self.model_path = os.path.join(cfg.model_path, "model")
            load_module(answer_model, self.model_path, cfg.answer_checkpoint)
            load_module(question_model, self.model_path, cfg.question_checkpoint)

        self.answer_model = answer_model
        self.answer_tokenizer = answer_tokenizer
        self.question_model = question_model
        self.question_tokenizer = question_tokenizer

    def question_greedy_predict(self, batch, current_device):
        """Greedily generate the questions and prepare inputs for the answer
        module."""
        question_input_ids = batch["entity_relation_passage_input_ids"]
        question_input_mask = batch["entity_relation_passage_attention_mask"]
        if self.config.gpu:
            question_input_ids = question_input_ids.to(current_device)
            question_input_mask = question_input_mask.to(current_device)

        question_predictions = self.question_model.generate(
            input_ids=question_input_ids,
            attention_mask=question_input_mask,
        )

        question_predictions_str = self.question_tokenizer.batch_decode(
            question_predictions, skip_special_tokens=True
        )

        new_articles = []
        for i in range(len(batch["passages"])):
            new_article = (
                "question: "
                + question_predictions_str[i]
                + " context: "
                + batch["passages"][i]
                + " </s>"
            )
            new_articles.append(new_article)

        answer_inputs = self.answer_tokenizer(
            new_articles,
            truncation=True,
            padding="max_length",
            max_length=self.config.source_max_length,
            add_special_tokens=False,
            return_tensors="pt",
        )
        answer_input_ids = answer_inputs.input_ids
        answer_input_mask = answer_inputs.attention_mask
        if self.config.gpu:
            answer_input_ids = answer_input_ids.to(current_device)
            answer_input_mask = answer_input_mask.to(current_device)

        return answer_input_ids, answer_input_mask

    def predict_step(self, batch):
        # Free memory in GPU, very important!
        clear_cache()
        # disable dropout
        self.answer_model.eval()
        self.question_model.eval()

        answer_input_ids, answer_input_mask = self.question_greedy_predict(batch)

        second_entity_predictions = self.answer_model.generate(
            input_ids=answer_input_ids,
            attention_mask=answer_input_mask,
        )
        second_entity_predictions_str = self.answer_tokenizer.batch_decode(
            second_entity_predictions, skip_special_tokens=True
        )
        for index in range(len(second_entity_predictions_str)):
            pred_str = second_entity_predictions_str[index]
            output_batch = {
                "predictions_str": pred_str,
            }
            yield output_batch

    def pgg_answer_training(self, batch, current_device):
        self.question_model.eval()
        self.question_optimizer.zero_grad()

        answer_input_ids, answer_input_mask = self.question_greedy_predict(
            batch, current_device
        )

        self.answer_model.train()
        self.answer_optimizer.zero_grad()

        target_mask = batch["second_entity_attention_mask"]
        labels = batch["second_entity_labels"]
        if self.config.gpu:
            target_mask = target_mask.to(current_device)
            labels = labels.to(current_device)

        output = self.answer_model(
            input_ids=answer_input_ids,
            attention_mask=answer_input_mask,
            decoder_attention_mask=target_mask,
            labels=labels,
        )

        re_loss = output.loss
        loss = re_loss
        loss_value = loss.item()

        if math.isnan(loss):
            return {"loss_value": loss_value}

        # BackProp
        loss.backward()

        # Optimize
        self.answer_optimizer.step()

        return {"loss_value": loss_value}

    def pg_question_training(self, batch, current_device):
        self.question_optimizer.zero_grad()
        self.answer_optimizer.zero_grad()
        self.question_model.train()
        self.answer_model.eval()

        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction="none")
        if self.config.gpu:
            loss_fct = loss_fct.to(current_device)

        # Loss from the entity relation examples!
        question_input_ids = batch["entity_relation_passage_input_ids"]
        question_input_mask = batch["entity_relation_passage_attention_mask"]
        if self.config.gpu:
            question_input_ids = question_input_ids.to(current_device)
            question_input_mask = question_input_mask.to(current_device)

        b_sz, _ = question_input_ids.size()

        # Use diverse beam search to collect samples.
        beam_question_outputs = self.question_model.generate(
            input_ids=question_input_ids,
            attention_mask=question_input_mask,
            no_repeat_ngram_size=self.config.no_repeat_ngram_size,
            early_stopping=self.config.early_stopping,
            max_length=self.config.decoder_max_length,
            num_return_sequences=self.config.num_beams,
            num_beams=self.config.num_beams,
            num_beam_groups=self.config.num_beam_groups,
            diversity_penalty=self.config.beam_diversity_penalty,
            output_scores=True,
            return_dict_in_generate=True,
        )

        beam_questions, beam_log_p = prob_of_sampled_predictions(
            loss_fct, beam_question_outputs
        )

        re_log_p_question = beam_log_p.view(self.config.num_beams, b_sz)

        beam_question_predictions_str = self.question_tokenizer.batch_decode(
            beam_questions, skip_special_tokens=True
        )

        beam_question_predictions_str_reshaped = [
            beam_question_predictions_str[
                i * (self.config.num_beams) : (i + 1) * (self.config.num_beams)
            ]
            for i in range(b_sz)
        ]

        new_articles = []
        for i in range(b_sz):
            for j in range(self.config.num_beams):
                new_article = (
                    "question: "
                    + beam_question_predictions_str_reshaped[i][j]
                    + " context: "
                    + batch["passages"][i]
                    + " </s>"
                )
                new_articles.append(new_article)

        answer_inputs = self.answer_tokenizer(
            new_articles,
            truncation=True,
            padding="max_length",
            max_length=self.config.source_max_length,
            add_special_tokens=False,
            return_tensors="pt",
        )

        answer_input_ids = answer_inputs.input_ids
        answer_input_mask = answer_inputs.attention_mask
        if self.config.gpu:
            answer_input_ids = answer_input_ids.to(current_device)
            answer_input_mask = answer_input_mask.to(current_device)

        target_mask = batch["second_entity_attention_mask"]
        labels = batch["second_entity_labels"]
        if self.config.gpu:
            target_mask = target_mask.to(current_device)
            labels = labels.to(current_device)

        b_sz, seq_len = labels.size()
        labels = labels.repeat(1, self.config.num_beams).view(-1, seq_len)
        target_mask = target_mask.repeat(1, self.config.num_beams).view(-1, seq_len)

        output = self.answer_model(
            input_ids=answer_input_ids,
            attention_mask=answer_input_mask,
            decoder_attention_mask=target_mask,
            decoder_input_ids=self.answer_model._shift_right(labels),
            labels=None,
        )

        log_p = -loss_fct(
            output.logits.view(-1, output.logits.size(-1)),
            labels.view(-1),
        )

        b, sz, v = output.logits.size()
        log_p = log_p.view(b, sz)
        good_log_p = log_p.masked_fill_(labels == -100, 0.0)
        answer_log_p = torch.sum(good_log_p, dim=1).squeeze()
        answer_log_p = answer_log_p.view(self.config.num_beams, b_sz)

        # Psuedo loss for the policy gradient.
        re_loss = -torch.mean(
            torch.mean(
                torch.mul(
                    torch.transpose(answer_log_p, 0, 1),
                    torch.transpose(re_log_p_question, 0, 1),
                ),
                dim=1,
            ),
            dim=0,
        )

        loss = re_loss
        loss_value = loss.item()

        if math.isnan(loss):
            return {"loss_value": loss_value}

        # BackProp
        loss.backward()

        # Optimize
        self.question_optimizer.step()

        return {"loss_value": loss_value}

    def train_step(
        self,
        batch,
        current_device,
        phase="answer",
    ):
        # Free memory in GPU, very important!
        clear_cache()
        # Turn on training mode which enables dropout.
        if phase == "answer":
            return self.pgg_answer_training(batch, current_device)

        elif phase == "question":
            return self.pg_question_training(batch, current_device)
