{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/snajafi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/snajafi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Blue Score.\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from src.train import normalize_answer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "from src.train import *\n",
    "from ast import literal_eval\n",
    "import datasets\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import nlp\n",
    "import random\n",
    "import numpy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 80.02699055330635, 'f1': 88.68562922717916}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(\"./t5_all/t5-epoch-on-all-squad.dev.predictions.txt\").astype(str)\n",
    "#df = pd.read_csv(\"./t5_all/t5-epoch-on-all-squad.dev.predictions.txt\").astype(str)\n",
    "#df = pd.read_csv(\"./t5_squad/t5-base-predictions.txt\").astype(str)\n",
    "#df = pd.read_csv(\"./t5_squad/t5-base-predictions.txt\").astype(str)\n",
    "predictions = df[\"predictions_str\"].tolist()\n",
    "normal_preds = [normalize_answer(pred) for pred in predictions]\n",
    "dev_refs = read_squad_refs(\"./squad/dev-v2.0.json\")\n",
    "normal_refs = []\n",
    "for row in dev_refs:\n",
    "    temp = []\n",
    "    for ref in row:\n",
    "        temp.append(normalize_answer(ref))\n",
    "    normal_refs.append(temp)\n",
    "\n",
    "evaluate(normal_refs, normal_preds)\n",
    "\n",
    "# Only on answerable parts of the squad 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pycocoevalcap.meteor.meteor import Meteor\n",
    "from src.pycocoevalcap.rouge.rouge import Rouge\n",
    "from src.pycocoevalcap.bleu.bleu import Bleu\n",
    "\n",
    "meteor_obj = Meteor()\n",
    "rouge_obj = Rouge()\n",
    "bleu_obj = Bleu(4)\n",
    "\n",
    "def nar_test_eval(file_name):\n",
    "    df = pd.read_csv(file_name).astype(str)\n",
    "    predictions = df[\"predictions_str\"].tolist()\n",
    "    normal_preds = [normalize_answer(pred).strip(\".\") for pred in predictions]\n",
    "\n",
    "    def process_all_rows(row):\n",
    "        \"\"\"Helper function.\"\"\"\n",
    "        all_answers = set()\n",
    "        for asw in row[\"answers\"]:\n",
    "            answer = asw[\"text\"]\n",
    "            all_answers.add(normalize_answer(answer).strip(\".\"))\n",
    "        return {\"all_answers\": list(all_answers)}\n",
    "\n",
    "    ref_dev_dataset = load_dataset(\"narrativeqa\", split=\"test\")\n",
    "    ref_dev_dataset = ref_dev_dataset.map(process_all_rows, remove_columns=[\"document\", \"answers\", \"question\"])\n",
    "    refs = [row[\"all_answers\"] for row in ref_dev_dataset]\n",
    "\n",
    "    word_target_dict = {}\n",
    "    word_response_dict = {}\n",
    "\n",
    "    for i in range(len(refs)):\n",
    "        if len(refs[i]) == 2:\n",
    "            word_target_dict[i] = [refs[i][0], refs[i][1]]\n",
    "        else:\n",
    "            word_target_dict[i] = [refs[i][0], refs[i][0]]\n",
    "        word_response_dict[i] = [normal_preds[i]]\n",
    "\n",
    "    bleu_score, bleu_scores = bleu_obj.compute_score(\n",
    "            word_target_dict, word_response_dict)\n",
    "\n",
    "    bleu1_score, _, _, bleu4_score = bleu_score\n",
    "\n",
    "    bleu1_scores, _, _, bleu4_scores = bleu_scores\n",
    "\n",
    "    #meteor_score, meteor_scores = meteor_obj.compute_score(\n",
    "    #        word_target_dict, word_response_dict)\n",
    "\n",
    "    rouge_score, rouge_scores = rouge_obj.compute_score(\n",
    "            word_target_dict, word_response_dict) \n",
    "\n",
    "    #print(\"ROUGE-L\", rouge_score)\n",
    "    #print(\"BLEU-1\", bleu1_score)\n",
    "    #print(\"BLEU-4\", bleu4_score)\n",
    "    #print(\"METEOR: \", meteor_score)\n",
    "    return {\"ROUGE-L\": rouge_score, \"BLEU-1\": bleu1_score, \"BLEU-4\": bleu4_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset narrative_qa (/home/snajafi/.cache/huggingface/datasets/narrative_qa/default/0.0.0/5571769afc7baf3132426991d9d84147d5b60c98320ab0b696e8c9ad2d712646)\n",
      "Loading cached processed dataset at /home/snajafi/.cache/huggingface/datasets/narrative_qa/default/0.0.0/5571769afc7baf3132426991d9d84147d5b60c98320ab0b696e8c9ad2d712646/cache-0199433687b12080.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 34290, 'reflen': 35134, 'guess': [34290, 23733, 16156, 11015], 'correct': [17671, 7661, 3260, 1441]}\n",
      "ratio: 0.9759776854328863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-L': 0.5317532522967355,\n",
       " 'BLEU-1': 0.5028102188346371,\n",
       " 'BLEU-4': 0.2511642820439916}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T5-base from google fine-tuned on NQ, RACE and Squad!\n",
    "nar_test_eval(\"./t5-all-nq.test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nar_test_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7ab8b69fa0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnar_test_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./t5-all-nq.test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Redo by lowercasing the input data!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nar_test_eval' is not defined"
     ]
    }
   ],
   "source": [
    "nar_test_eval(\"./t5-all-nq.test.csv\")\n",
    "\n",
    "# Redo by lowercasing the input data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset narrative_qa (/home/snajafi/.cache/huggingface/datasets/narrative_qa/default/0.0.0/5571769afc7baf3132426991d9d84147d5b60c98320ab0b696e8c9ad2d712646)\n",
      "Loading cached processed dataset at /home/snajafi/.cache/huggingface/datasets/narrative_qa/default/0.0.0/5571769afc7baf3132426991d9d84147d5b60c98320ab0b696e8c9ad2d712646/cache-752c0030374476b8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.387799564270395\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import rouge\n",
    "\n",
    "rouge_l_evaluator = rouge.Rouge(\n",
    "    metrics=[\"rouge-l\"],\n",
    "    max_n=4,\n",
    "    limit_length=True,\n",
    "    length_limit=128,\n",
    "    length_limit_type=\"words\",\n",
    "    apply_avg=True,\n",
    "    apply_best=True,\n",
    "    alpha=0.5,\n",
    "    weight_factor=1.2,\n",
    "    stemming=True,\n",
    ")\n",
    "\n",
    "def rouge_l(p, g):\n",
    "    return rouge_l_evaluator.get_scores(p, g)\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, [ground_truth])\n",
    "        scores_for_ground_truths.append(score)\n",
    "    if isinstance(score, dict) and \"rouge-l\" in score:\n",
    "        max_score = copy.deepcopy(score)\n",
    "        max_score[\"rouge-l\"][\"f\"] = round(\n",
    "            max([score[\"rouge-l\"][\"f\"] for score in scores_for_ground_truths]), 2\n",
    "        )\n",
    "        max_score[\"rouge-l\"][\"p\"] = round(\n",
    "            max([score[\"rouge-l\"][\"p\"] for score in scores_for_ground_truths]), 2\n",
    "        )\n",
    "        max_score[\"rouge-l\"][\"r\"] = round(\n",
    "            max([score[\"rouge-l\"][\"r\"] for score in scores_for_ground_truths]), 2\n",
    "        )\n",
    "        return max_score\n",
    "    else:\n",
    "        return round(max(scores_for_ground_truths), 2)\n",
    "\n",
    "df = pd.read_csv(\"./t5-all-nq.test.csv\").astype(str)\n",
    "predictions = df[\"predictions_str\"].tolist()\n",
    "normal_preds = [normalize_answer(pred).strip(\".\") for pred in predictions]\n",
    "\n",
    "def process_all_rows(row):\n",
    "    \"\"\"Helper function.\"\"\"\n",
    "    all_answers = set()\n",
    "    for asw in row[\"answers\"]:\n",
    "        answer = asw[\"text\"]\n",
    "        all_answers.add(normalize_answer(answer).strip(\".\"))\n",
    "    return {\"all_answers\": list(all_answers)}\n",
    "\n",
    "ref_dev_dataset = load_dataset(\"narrativeqa\", split=\"test\")\n",
    "ref_dev_dataset = ref_dev_dataset.map(process_all_rows, remove_columns=[\"document\", \"answers\", \"question\"])\n",
    "refs = [row[\"all_answers\"] for row in ref_dev_dataset]\n",
    "\n",
    "scores = []\n",
    "for i in range(len(refs)):\n",
    "    golds_subset = refs[i]\n",
    "    rouge_l_score = metric_max_over_ground_truths(rouge_l, normal_preds[i], golds_subset)\n",
    "    scores.append(rouge_l_score[\"rouge-l\"][\"f\"])\n",
    "\n",
    "print(100.0 * sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.14349999999955\n",
      "['caribbean']\n",
      "cobra and san pero\n",
      "4268\n",
      "6000\n",
      "4268\n",
      "1732\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import rouge\n",
    "from ast import literal_eval\n",
    "\n",
    "rouge_l_evaluator = rouge.Rouge(\n",
    "    metrics=[\"rouge-l\"],\n",
    "    max_n=4,\n",
    "    limit_length=True,\n",
    "    length_limit=128,\n",
    "    length_limit_type=\"words\",\n",
    "    apply_avg=True,\n",
    "    apply_best=True,\n",
    "    alpha=0.5,\n",
    "    weight_factor=1.2,\n",
    "    stemming=True,\n",
    ")\n",
    "\n",
    "def rouge_l(p, g):\n",
    "    return rouge_l_evaluator.get_scores(p, g)\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, [ground_truth])\n",
    "        scores_for_ground_truths.append(score)\n",
    "    if isinstance(score, dict) and \"rouge-l\" in score:\n",
    "        max_score = copy.deepcopy(score)\n",
    "        max_score[\"rouge-l\"][\"f\"] = round(\n",
    "            max([score[\"rouge-l\"][\"f\"] for score in scores_for_ground_truths]), 2\n",
    "        )\n",
    "        max_score[\"rouge-l\"][\"p\"] = round(\n",
    "            max([score[\"rouge-l\"][\"p\"] for score in scores_for_ground_truths]), 2\n",
    "        )\n",
    "        max_score[\"rouge-l\"][\"r\"] = round(\n",
    "            max([score[\"rouge-l\"][\"r\"] for score in scores_for_ground_truths]), 2\n",
    "        )\n",
    "        return max_score\n",
    "    else:\n",
    "        return round(max(scores_for_ground_truths), 2)\n",
    "\n",
    "df = pd.read_csv(\"./test.0.processed.predicted.csv\").astype(str)\n",
    "predictions = df[\"predictions_str\"].tolist()\n",
    "normal_preds = [normalize_answer(pred).strip(\".\") for pred in predictions]\n",
    "\n",
    "ref_df = pd.read_csv(\"./test.0.processed.csv\").astype(str)\n",
    "refs = ref_df[\"correctAnswers\"].tolist()\n",
    "normal_refs = [[normalize_answer(answer).strip(\".\") for answer in literal_eval(ref)] for ref in refs]\n",
    "\n",
    "\n",
    "correct_indices = []\n",
    "wrong_indices = []\n",
    "empty_indices = []\n",
    "\n",
    "scores = []\n",
    "for i in range(len(normal_refs)):\n",
    "    golds_subset = normal_refs[i]\n",
    "    if not golds_subset:\n",
    "        empty_indices.append(i)\n",
    "        continue\n",
    "    elif normal_preds[i] in golds_subset:\n",
    "        correct_indices.append(i)\n",
    "    elif normal_preds[i] not in golds_subset:\n",
    "        wrong_indices.append(i)\n",
    "\n",
    "    rouge_l_score = metric_max_over_ground_truths(rouge_l, normal_preds[i], golds_subset)\n",
    "    scores.append(rouge_l_score[\"rouge-l\"][\"f\"])\n",
    "\n",
    "print(100.0 * sum(scores) / len(scores))\n",
    "\n",
    "answerable_refs = []\n",
    "answerable_preds = []\n",
    "un_answerable_preds = []\n",
    "for i in range(len(normal_refs)):\n",
    "    golds_subset = normal_refs[i]\n",
    "    if golds_subset:\n",
    "        answerable_refs.append(golds_subset)\n",
    "        answerable_preds.append(normal_preds[i])\n",
    "    else:\n",
    "        un_answerable_preds.append(normal_preds[i])\n",
    "\n",
    "evaluate(answerable_refs, answerable_preds)\n",
    "\n",
    "\n",
    "in_complete_data = {\"ref\": [], \"pred\": []}\n",
    "complete_data = {\"ref\": [], \"pred\": []}\n",
    "\n",
    "for i in range(len(answerable_refs)):\n",
    "    ans_ref = answerable_refs[i]\n",
    "    ans_pred = answerable_preds[i]\n",
    "    if ans_pred not in ans_ref:\n",
    "        in_complete_data[\"ref\"].append(ans_ref)\n",
    "        in_complete_data[\"pred\"].append(ans_pred)\n",
    "    else:\n",
    "        complete_data[\"ref\"].append(ans_ref)\n",
    "        complete_data[\"pred\"].append(ans_pred)\n",
    "\n",
    "print(in_complete_data[\"ref\"][10])\n",
    "print(in_complete_data[\"pred\"][10])\n",
    "\n",
    "print(len(complete_data[\"ref\"]))\n",
    "print(len(un_answerable_preds))\n",
    "\n",
    "\n",
    "print(len(correct_indices))\n",
    "print(len(wrong_indices))\n",
    "print(len(empty_indices))\n",
    "\n",
    "\n",
    "correct_samples = random.sample(correct_indices, 10)\n",
    "correct_rows_sample = ref_df.iloc[correct_samples,]\n",
    "correct_rows_sample_predictions = df.iloc[correct_samples,]\n",
    "\n",
    "wrong_samples = random.sample(wrong_indices, 10)\n",
    "wrong_rows_sample = ref_df.iloc[wrong_samples,]\n",
    "wrong_rows_sample_predictions = df.iloc[wrong_samples,]\n",
    "\n",
    "empty_samples = random.sample(empty_indices, 10)\n",
    "empty_rows_sample = ref_df.iloc[empty_samples,]\n",
    "empty_rows_sample_predictions = df.iloc[empty_samples,]\n",
    "\n",
    "correct_rows_sample.to_csv(\"./correct_rows_sample.csv\")\n",
    "correct_rows_sample_predictions.to_csv(\"./correct_rows_sample.predictions.csv\")\n",
    "\n",
    "\n",
    "wrong_rows_sample.to_csv(\"./wrong_rows_sample.csv\")\n",
    "wrong_rows_sample_predictions.to_csv(\"./wrong_rows_sample.predictions.csv\")\n",
    "\n",
    "empty_rows_sample.to_csv(\"./empty_rows_sample.csv\")\n",
    "empty_rows_sample_predictions.to_csv(\"./empty_rows_sample.predictions.csv\")\n",
    "#print(wrong_rows_sample)\n",
    "#print(empty_rows_sample)\n",
    "\n",
    "#print(random.sample(wrong_indices, 10))\n",
    "#print(random.sample(empty_indices, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34203120798658615\n",
      "{'rouge-1': {'r': 0.0043, 'r_conf_int': (0.00038, 0.00947), 'p': 0.00038, 'p_conf_int': (2e-05, 0.00082), 'f': 0.00069, 'f_conf_int': (5e-05, 0.00148)}, 'rouge-2': {'r': 0.0, 'r_conf_int': (0.0, 0.0), 'p': 0.0, 'p_conf_int': (0.0, 0.0), 'f': 0.0, 'f_conf_int': (0.0, 0.0)}, 'rouge-l': {'r': 0.0043, 'r_conf_int': (0.00038, 0.00947), 'p': 0.00038, 'p_conf_int': (2e-05, 0.00082), 'f': 0.00069, 'f_conf_int': (5e-05, 0.00148)}}\n",
      "{'rouge-1': {'r': 0.00507, 'r_conf_int': (0.00299, 0.0075), 'p': 0.00047, 'p_conf_int': (0.00027, 0.0007), 'f': 0.00085, 'f_conf_int': (0.00049, 0.00127)}, 'rouge-2': {'r': 0.0, 'r_conf_int': (0.0, 0.0), 'p': 0.0, 'p_conf_int': (0.0, 0.0), 'f': 0.0, 'f_conf_int': (0.0, 0.0)}, 'rouge-l': {'r': 0.00507, 'r_conf_int': (0.00299, 0.0075), 'p': 0.00047, 'p_conf_int': (0.00027, 0.0007), 'f': 0.00085, 'f_conf_int': (0.00049, 0.00127)}}\n",
      "0.39351543257466154\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(\"./t5_on_dream/t5-base-with-nq-dream-inferentials-predictions.txt\").astype(str)\n",
    "\n",
    "df = pd.read_csv(\"./unifiedqa-inferentials.csv\").astype(str)\n",
    "predictions = df[\"predictions_str\"].tolist()\n",
    "normal_preds = [normalize_answer(pred) for pred in predictions]\n",
    "val_contexts, val_answers = read_dream_data('./dream-data-v2/summary-passages-questions-cleaned.inferentials.csv')\n",
    "refs = [normalize_answer(literal_eval(answer)[0]) for answer in val_answers]\n",
    "\n",
    "rouge = PerlRouge(rouge_l=True)\n",
    "\n",
    "scores = rouge.evaluate(normal_preds, refs)\n",
    "\n",
    "model = SentenceTransformer('stsb-roberta-large')\n",
    "\n",
    "embedding1 = model.encode(refs, convert_to_tensor=True)\n",
    "embedding2 = model.encode(normal_preds, convert_to_tensor=True)\n",
    "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "sim = 0.0\n",
    "for i in range(len(refs)):\n",
    "    sim += cosine_scores[i][i].item()\n",
    "\n",
    "mean_sim = sim / len(refs)\n",
    "\n",
    "print(mean_sim)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "\n",
    "\n",
    "#df = pd.read_csv(\"./t5_on_dream/t5-base-with-nq-dream-literal-predictions.txt\").astype(str)\n",
    "df = pd.read_csv(\"./unifiedqa-literals.csv\").astype(str)\n",
    "predictions = df[\"predictions_str\"].tolist()\n",
    "normal_preds = [normalize_answer(pred) for pred in predictions]\n",
    "val_contexts, val_answers = read_dream_data('./dream-data-v2/summary-passages-questions-cleaned.literals.csv')\n",
    "refs = [normalize_answer(literal_eval(answer)[0]) for answer in val_answers]\n",
    "\n",
    "rouge = PerlRouge(rouge_l=True)\n",
    "\n",
    "scores = rouge.evaluate(normal_preds, refs)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "\n",
    "model = SentenceTransformer('stsb-roberta-large')\n",
    "\n",
    "embedding1 = model.encode(refs, convert_to_tensor=True)\n",
    "embedding2 = model.encode(normal_preds, convert_to_tensor=True)\n",
    "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "sim = 0.0\n",
    "for i in range(len(refs)):\n",
    "    sim += cosine_scores[i][i].item()\n",
    "\n",
    "mean_sim = sim / len(refs)\n",
    "\n",
    "print(mean_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
