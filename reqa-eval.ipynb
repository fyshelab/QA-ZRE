{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on http://nlp.cs.washington.edu/zeroshot/evaluate.py\n",
    "import pandas as pd\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "PUNCTUATION = set(string.punctuation)\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_latin(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "\n",
    "def unk_zero_re_eval(test_file, answer_file):\n",
    "    q_aprf = unk_read_results(test_file, answer_file)\n",
    "    return pretify(q_aprf)\n",
    "\n",
    "def unk_read_results(test_set, answer_file):\n",
    "    with codecs.open(test_set, \"r\", \"utf-8\") as fin:\n",
    "        data = [line.strip().split(\"\\t\") for line in fin]\n",
    "    metadata = [x[:4] for x in data]\n",
    "    gold = [set(x[4:]) for x in data]\n",
    "\n",
    "    with codecs.open(answer_file, \"r\", \"utf-8\") as fin:\n",
    "        answers = [line.strip() for line in fin]\n",
    "\n",
    "    new_answers = []\n",
    "    for answer in answers[1:]:\n",
    "        if answer != \"no_answer\":\n",
    "            new_answers.append(answer)\n",
    "        else:\n",
    "            new_answers.append(\"\")\n",
    "\n",
    "    telemetry = []\n",
    "    for m, g, a in zip(metadata, new_gold, new_answers):\n",
    "        stats = score(g, a)\n",
    "        telemetry.append([m[0], m[1], str(len(g) > 0), stats])\n",
    "    return aprf(telemetry)\n",
    "\n",
    "def parse_no_answers(results):\n",
    "    p_answer = [\n",
    "        a for i, a in sorted([(int(i), a) for i, a in results[0][\"scores\"].items()])\n",
    "    ]\n",
    "    p_no_answer = [\n",
    "        a for i, a in sorted([(int(i), a) for i, a in results[0][\"na\"].items()])\n",
    "    ]\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    return [answer > no_answer for answer, no_answer in zip(p_answer, p_no_answer)]\n",
    "\n",
    "\n",
    "def gb(collection, keyfunc):\n",
    "    return [(k, list(g)) for k, g in groupby(sorted(collection, key=keyfunc), keyfunc)]\n",
    "\n",
    "\n",
    "def aprf(g):\n",
    "    tp, tn, sys_pos, real_pos = sum(map(lambda x: x[-1], g))\n",
    "    total = len(g)\n",
    "    # a = float(tp + tn) / total\n",
    "    # nr = tn / float(total - real_pos)\n",
    "    # npr = tn / float(total - sys_pos)\n",
    "    if tp == 0:\n",
    "        p = r = f = 0.0\n",
    "    else:\n",
    "        p = tp / float(sys_pos)\n",
    "        r = tp / float(real_pos)\n",
    "        f = 2 * p * r / (p + r)\n",
    "    # return np.array((a, p, r, f, npr, nr))\n",
    "    return np.array((p, r, f))\n",
    "\n",
    "\n",
    "def score(gold, answer):\n",
    "    if len(gold) > 0:\n",
    "        gold = set.union(*[simplify(g) for g in gold])\n",
    "    answer = simplify(answer)\n",
    "    result = np.zeros(4)\n",
    "    if answer == gold:\n",
    "        if len(gold) > 0:\n",
    "            result[0] += 1\n",
    "        else:\n",
    "            result[1] += 1\n",
    "    if len(answer) > 0:\n",
    "        result[2] += 1\n",
    "    if len(gold) > 0:\n",
    "        result[3] += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def simplify(answer):\n",
    "    return set(\n",
    "        \"\".join(c for c in t if c not in PUNCTUATION)\n",
    "        for t in answer.strip().lower().split()\n",
    "    ) - {\"the\", \"a\", \"an\", \"and\", \"\"}\n",
    "\n",
    "\n",
    "def pretify(results):\n",
    "    return \" \\t \".join(\n",
    "        [\n",
    "            \": \".join((k, v))\n",
    "            for k, v in zip(\n",
    "                [\"Precision\", \"Recall\", \"F1\"],\n",
    "                map(lambda r: \"{0:.2f}%\".format(r * 100), results),\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_the_prediction_files(main_path, list_of_files):\n",
    "    for file in list_of_files:\n",
    "        df = pd.read_csv(os.path.join(main_path, file), sep=',')\n",
    "        df[\"predictions_str\"].to_csv(os.path.join(\"/tmp/\", file), sep='\\t', header=True, index=False)\n",
    "\n",
    "def unk_eval_the_prediction_files(list_of_files, gold_file):\n",
    "    scores = {}\n",
    "    scores_list = []\n",
    "    for file in list_of_files:\n",
    "        score = unk_zero_re_eval(gold_file, os.path.join(\"/tmp/\", file))\n",
    "        f1_score = float(score.split()[-1][0:-1])\n",
    "        scores[f1_score] = file\n",
    "        scores_list.append(f1_score)\n",
    "\n",
    "    f1s = np.array(scores_list)\n",
    "    max_f1 = max(scores.keys())\n",
    "    return scores[max_f1],  max_f1, f1s, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 gold_fold.1.dev.predictions.step.800.csv 56.83\n",
      "#\n",
      "2 gold_fold.2.dev.predictions.step.2000.csv 65.76\n",
      "#\n",
      "3 gold_fold.3.dev.predictions.step.4200.csv 63.59\n",
      "#\n",
      "4 gold_fold.4.dev.predictions.step.1400.csv 64.44\n",
      "#\n",
      "5 gold_fold.5.dev.predictions.step.900.csv 67.0\n",
      "#\n",
      "6 gold_fold.6.dev.predictions.step.400.csv 69.45\n",
      "#\n",
      "7 gold_fold.7.dev.predictions.step.6100.csv 59.3\n",
      "#\n",
      "8 gold_fold.8.dev.predictions.step.7300.csv 64.1\n",
      "#\n",
      "9 gold_fold.9.dev.predictions.step.1800.csv 67.48\n",
      "#\n",
      "10 gold_fold.10.dev.predictions.step.4100.csv 61.41\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the dev predictions on the RE-QA dataset using the model having access to gold templates!\n",
    "for fold_i in range(1, 11, 1):\n",
    "    fold_gold_file = \"./zero-shot-extraction/relation_splits/dev.{}\".format(fold_i-1)\n",
    "    fold_path = \"/home/saeednjf/scratch/feb-15-2022-arr/fold_{}/gold/dev_predictions/\".format(fold_i)\n",
    "    fold_files = [\"gold_fold.{}.dev.predictions.step.{}.csv\".format(fold_i, 100 * i) for i in range(1, 506, 1)]\n",
    "    preprocess_the_prediction_files(fold_path, fold_files)\n",
    "    max_file,  max_f1, f1s, scores = unk_eval_the_prediction_files(fold_files, fold_gold_file)\n",
    "    print(fold_i, max_file, max_f1)\n",
    "    print(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 concat_fold.1.dev.predictions.step.1700.csv 53.96\n",
      "#\n",
      "2 concat_fold.2.dev.predictions.step.4600.csv 64.46\n",
      "#\n",
      "3 concat_fold.3.dev.predictions.step.18600.csv 61.4\n",
      "#\n",
      "4 concat_fold.4.dev.predictions.step.1400.csv 67.12\n",
      "#\n",
      "5 concat_fold.5.dev.predictions.step.7900.csv 69.91\n",
      "#\n",
      "6 concat_fold.6.dev.predictions.step.2600.csv 64.16\n",
      "#\n",
      "7 concat_fold.7.dev.predictions.step.24700.csv 63.0\n",
      "#\n",
      "8 concat_fold.8.dev.predictions.step.13800.csv 61.08\n",
      "#\n",
      "9 concat_fold.9.dev.predictions.step.17700.csv 61.6\n",
      "#\n",
      "10 concat_fold.10.dev.predictions.step.35500.csv 57.72\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the dev predictions on the RE-QA dataset using the model having access to psuedo questions.\n",
    "for fold_i in range(1, 11, 1):\n",
    "    fold_gold_file = \"./zero-shot-extraction/relation_splits/dev.{}\".format(fold_i-1)\n",
    "    fold_path = \"/home/saeednjf/scratch/feb-15-2022-arr/fold_{}/concat/dev_predictions/\".format(fold_i)\n",
    "    fold_files = [\"concat_fold.{}.dev.predictions.step.{}.csv\".format(fold_i, 100 * i) for i in range(1, 506, 1)]\n",
    "    preprocess_the_prediction_files(fold_path, fold_files)\n",
    "    max_file,  max_f1, f1s, scores = unk_eval_the_prediction_files(fold_files, fold_gold_file)\n",
    "    print(fold_i, max_file, max_f1)\n",
    "    print(\"#\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
