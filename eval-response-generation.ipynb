{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Average F1 from the squad v2 evaluation script.\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def removesuffix(self: str, suffix: str) -> str:\n",
    "    if self.endswith(suffix):\n",
    "        return self[:-len(suffix)]\n",
    "    else:\n",
    "        return self[:]\n",
    "\n",
    "def read_pred_file(pred_file_name):\n",
    "    df = pd.read_csv(pred_file_name).astype(str)\n",
    "    predictions = df[\"predictions_str\"].tolist()\n",
    "    normal_preds = [removesuffix(pred, ' </s>') for pred in predictions]\n",
    "    return normal_preds\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def read_squad_refs(path):\n",
    "    path = Path(path)\n",
    "    with open(path, \"rb\") as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    all_refs = []\n",
    "    for group in squad_dict[\"data\"]:\n",
    "        for passage in group[\"paragraphs\"]:\n",
    "            for qa in passage[\"qas\"]:\n",
    "                gold_answers = [a['text'] for a in qa['answers'] if normalize_answer(a['text'])]\n",
    "                if not gold_answers:\n",
    "                  # For unanswerable questions, only correct answer is empty string\n",
    "                  gold_answers = ['']\n",
    "                all_refs.append(gold_answers)\n",
    "\n",
    "    return all_refs\n",
    "\n",
    "def get_raw_scores(squad_path, preds):\n",
    "    exact_scores = {}\n",
    "    f1_scores = {}\n",
    "    all_refs = read_squad_refs(squad_path)\n",
    "    for i, gold_answers in enumerate(all_refs):\n",
    "        a_pred = preds[i]\n",
    "        if a_pred == 'no_answer':\n",
    "            a_pred = ''\n",
    "\n",
    "        # Take max over all gold answers\n",
    "        exact_scores[i] = max(compute_exact(a, a_pred) for a in gold_answers)\n",
    "        f1_scores[i] = max(compute_f1(a, a_pred) for a in gold_answers)\n",
    "    \n",
    "    mean_f1 = 100.0 * sum(f1_scores[k] for k in range(len(all_refs))) / len(all_refs)\n",
    "    return exact_scores, f1_scores, mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.38645187660664\n"
     ]
    }
   ],
   "source": [
    "preds = read_pred_file(\"~/t5-small-exps/naacl-2022/response_pretrained_model/squad_dev.epoch1.csv\")\n",
    "exact_scores, f1_scores, mean_f1 = get_raw_scores(\"./squad/dev-v2.0.json\", preds)\n",
    "print(mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.80907441422468\n"
     ]
    }
   ],
   "source": [
    "preds = read_pred_file(\"~/t5-small-exps/naacl-2022/response_pretrained_model/squad_dev.epoch2.csv\")\n",
    "exact_scores, f1_scores, mean_f1 = get_raw_scores(\"./squad/dev-v2.0.json\", preds)\n",
    "print(mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.1067892196832\n"
     ]
    }
   ],
   "source": [
    "preds = read_pred_file(\"~/t5-small-exps/naacl-2022/response_pretrained_model/squad_dev.epoch3.csv\")\n",
    "exact_scores, f1_scores, mean_f1 = get_raw_scores(\"./squad/dev-v2.0.json\", preds)\n",
    "print(mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.65155496438804\n"
     ]
    }
   ],
   "source": [
    "preds = read_pred_file(\"~/t5-small-exps/naacl-2022/response_pretrained_model/squad_dev.epoch4.csv\")\n",
    "exact_scores, f1_scores, mean_f1 = get_raw_scores(\"./squad/dev-v2.0.json\", preds)\n",
    "print(mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.18511206916718\n"
     ]
    }
   ],
   "source": [
    "preds = read_pred_file(\"~/t5-small-exps/naacl-2022/response_pretrained_model/squad_dev.unifiedqa.csv\")\n",
    "exact_scores, f1_scores, mean_f1 = get_raw_scores(\"./squad/dev-v2.0.json\", preds)\n",
    "print(mean_f1)\n",
    "\n",
    "# unified QA doesn't handle unknown answers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
